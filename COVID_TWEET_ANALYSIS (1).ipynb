{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SQLContext \n",
    "\n",
    "from pyspark.ml import Pipeline \n",
    "\n",
    "# SPARK-DEFAULT-CONF VARIABLES SETTING\n",
    "conf = pyspark.SparkConf().setAll([('spark.executor.memory', '16g'), \n",
    "                                   ('spark.executor.cores', '1'), \n",
    "                                   ('spark.cores.max', '1'), \n",
    "                                   ('spark.driver.memory','16g')])\n",
    "sc = SparkContext.getOrCreate(conf = conf) #Initialize the spark context\n",
    "sqlContext = SQLContext.getOrCreate(sc) #Create an SQL Context\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate() # Intializing a spark session at local instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------------+----------+--------------------+---------+\n",
      "|UserName|  ScreenName|            Location|   TweetAt|       OriginalTweet|Sentiment|\n",
      "+--------+------------+--------------------+----------+--------------------+---------+\n",
      "|    3799|       48751|              London|16-03-2020|@MeNyrbie @Phil_G...|  Neutral|\n",
      "|    3800|       48752|                  UK|16-03-2020|advice Talk to yo...| Positive|\n",
      "|    3801|       48753|           Vagabonds|16-03-2020|Coronavirus Austr...| Positive|\n",
      "|    3802|       48754|                null|16-03-2020|My food stock is ...|     null|\n",
      "|  PLEASE| don't panic| THERE WILL BE EN...|      null|                null|     null|\n",
      "+--------+------------+--------------------+----------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.load(\"file:///home/nakul/Downloads/Corona_NLP_train.csv\",\n",
    "                         format=\"com.databricks.spark.csv\",header=True,inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserName : 11150\n",
      "ScreenName : 20019\n",
      "Location : 35678\n",
      "TweetAt : 30186\n",
      "OriginalTweet : 30452\n",
      "Sentiment : 39745\n"
     ]
    }
   ],
   "source": [
    "# CHECKING FOR ALL THE NULL VALUES PRESENT IN THE DATA SEQUENTIALLY\n",
    "from pyspark.sql.functions import col\n",
    "for i in df.columns:\n",
    "    print(i,\":\",df.where(col(i).isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16288"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DROPPING THE NULL VALUES\n",
    "df=df.na.drop()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+----------+--------------------+------------------+\n",
      "|UserName|ScreenName|            Location|   TweetAt|       OriginalTweet|         Sentiment|\n",
      "+--------+----------+--------------------+----------+--------------------+------------------+\n",
      "|    3799|     48751|              London|16-03-2020|@MeNyrbie @Phil_G...|           Neutral|\n",
      "|    3800|     48752|                  UK|16-03-2020|advice Talk to yo...|          Positive|\n",
      "|    3801|     48753|           Vagabonds|16-03-2020|Coronavirus Austr...|          Positive|\n",
      "|    3804|     48756|ÜT: 36.319708,-82...|16-03-2020|As news of the re...|          Positive|\n",
      "|    3805|     48757|35.926541,-78.753267|16-03-2020|\"Cashier at groce...|          Positive|\n",
      "|    3807|     48759|     Atlanta, GA USA|16-03-2020|Due to COVID-19 o...|          Positive|\n",
      "|    3808|     48760|    BHAVNAGAR,GUJRAT|16-03-2020|For corona preven...|          Negative|\n",
      "|    3809|     48761|      Makati, Manila|16-03-2020|All month there h...|           Neutral|\n",
      "|    3811|     48763|          Horningsea|16-03-2020|#horningsea is a ...|Extremely Positive|\n",
      "|    3818|     48770|          Denver, CO|16-03-2020|For those who are...|          Positive|\n",
      "|    3819|     48771|southampton soxx xxx|16-03-2020|with 100  nations...|Extremely Negative|\n",
      "|    3823|     48775|    Downstage centre|16-03-2020|@10DowningStreet ...|          Negative|\n",
      "|    3824|     48776|              London|16-03-2020|UK #consumer poll...|Extremely Positive|\n",
      "|    3825|     48777|      Ketchum, Idaho|16-03-2020|In preparation fo...|          Negative|\n",
      "|    3826|     48778| Everywhere You Are!|16-03-2020|This morning I te...|Extremely Negative|\n",
      "|    3834|     48786|             Sverige|16-03-2020|Went to the super...|           Neutral|\n",
      "|    3836|     48788|              Canada|16-03-2020|Worried about the...|          Positive|\n",
      "|    3838|     48790|       United States|16-03-2020|Now I can go to t...|          Positive|\n",
      "|    3841|     48793|             Houston|16-03-2020|CHECK VIDEO ?? ht...|Extremely Negative|\n",
      "|    3842|     48794|Vancouver, Britis...|16-03-2020|Breaking Story: O...|           Neutral|\n",
      "+--------+----------+--------------------+----------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+----------+--------------------+------------------+----------------+\n",
      "|UserName|ScreenName|            Location|   TweetAt|       OriginalTweet|         Sentiment|target_Sentiment|\n",
      "+--------+----------+--------------------+----------+--------------------+------------------+----------------+\n",
      "|    3799|     48751|              London|16-03-2020|@MeNyrbie @Phil_G...|           Neutral|         Neutral|\n",
      "|    3800|     48752|                  UK|16-03-2020|advice Talk to yo...|          Positive|        Positive|\n",
      "|    3801|     48753|           Vagabonds|16-03-2020|Coronavirus Austr...|          Positive|        Positive|\n",
      "|    3804|     48756|ÜT: 36.319708,-82...|16-03-2020|As news of the re...|          Positive|        Positive|\n",
      "|    3805|     48757|35.926541,-78.753267|16-03-2020|\"Cashier at groce...|          Positive|        Positive|\n",
      "|    3807|     48759|     Atlanta, GA USA|16-03-2020|Due to COVID-19 o...|          Positive|        Positive|\n",
      "|    3808|     48760|    BHAVNAGAR,GUJRAT|16-03-2020|For corona preven...|          Negative|        Negative|\n",
      "|    3809|     48761|      Makati, Manila|16-03-2020|All month there h...|           Neutral|         Neutral|\n",
      "|    3811|     48763|          Horningsea|16-03-2020|#horningsea is a ...|Extremely Positive|        Positive|\n",
      "|    3818|     48770|          Denver, CO|16-03-2020|For those who are...|          Positive|        Positive|\n",
      "|    3819|     48771|southampton soxx xxx|16-03-2020|with 100  nations...|Extremely Negative|        Negative|\n",
      "|    3823|     48775|    Downstage centre|16-03-2020|@10DowningStreet ...|          Negative|        Negative|\n",
      "|    3824|     48776|              London|16-03-2020|UK #consumer poll...|Extremely Positive|        Positive|\n",
      "|    3825|     48777|      Ketchum, Idaho|16-03-2020|In preparation fo...|          Negative|        Negative|\n",
      "|    3826|     48778| Everywhere You Are!|16-03-2020|This morning I te...|Extremely Negative|        Negative|\n",
      "|    3834|     48786|             Sverige|16-03-2020|Went to the super...|           Neutral|         Neutral|\n",
      "|    3836|     48788|              Canada|16-03-2020|Worried about the...|          Positive|        Positive|\n",
      "|    3838|     48790|       United States|16-03-2020|Now I can go to t...|          Positive|        Positive|\n",
      "|    3841|     48793|             Houston|16-03-2020|CHECK VIDEO ?? ht...|Extremely Negative|        Negative|\n",
      "|    3842|     48794|Vancouver, Britis...|16-03-2020|Breaking Story: O...|           Neutral|         Neutral|\n",
      "+--------+----------+--------------------+----------+--------------------+------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "tweetDF1 = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEALING WITH THE IMPORTANT CATEGORICAL VALUES\n",
    "clean_val = {\"Neutral\":\"Neutral\",\"Positive\":\"Positive\",\"Negative\":\"Negative\",\n",
    "             \"Extremely Positive\":\"Positive\",\"Extremely Negative\":\"Negative\"}\n",
    "tweetDF1 = tweetDF1.replace(clean_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF=spark.createDataFrame(tweetDF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF=DF.withColumn(\"target\", DF[\"Sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+----------+--------------------+---------+----------------+--------+\n",
      "|UserName|ScreenName|Location|   TweetAt|       OriginalTweet|Sentiment|target_Sentiment|  target|\n",
      "+--------+----------+--------+----------+--------------------+---------+----------------+--------+\n",
      "|    3799|     48751|  London|16-03-2020|@MeNyrbie @Phil_G...|  Neutral|         Neutral| Neutral|\n",
      "|    3800|     48752|      UK|16-03-2020|advice Talk to yo...| Positive|        Positive|Positive|\n",
      "+--------+----------+--------+----------+--------------------+---------+----------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING THE FEATURES THAT ARE NOT TO BE USED\n",
    "DF=DF.drop(DF.TweetAt)\n",
    "DF=DF.drop(DF.UserName)\n",
    "DF=DF.drop(DF.ScreenName)\n",
    "DF=DF.drop(DF.Sentiment)\n",
    "DF=DF.drop(DF.Location)\n",
    "data = DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------+\n",
      "|       OriginalTweet|target_Sentiment|  target|\n",
      "+--------------------+----------------+--------+\n",
      "|@MeNyrbie @Phil_G...|         Neutral| Neutral|\n",
      "|advice Talk to yo...|        Positive|Positive|\n",
      "|Coronavirus Austr...|        Positive|Positive|\n",
      "|As news of the re...|        Positive|Positive|\n",
      "|\"Cashier at groce...|        Positive|Positive|\n",
      "|Due to COVID-19 o...|        Positive|Positive|\n",
      "|For corona preven...|        Negative|Negative|\n",
      "|All month there h...|         Neutral| Neutral|\n",
      "|#horningsea is a ...|        Positive|Positive|\n",
      "|For those who are...|        Positive|Positive|\n",
      "|with 100  nations...|        Negative|Negative|\n",
      "|@10DowningStreet ...|        Negative|Negative|\n",
      "|UK #consumer poll...|        Positive|Positive|\n",
      "|In preparation fo...|        Negative|Negative|\n",
      "|This morning I te...|        Negative|Negative|\n",
      "|Went to the super...|         Neutral| Neutral|\n",
      "|Worried about the...|        Positive|Positive|\n",
      "|Now I can go to t...|        Positive|Positive|\n",
      "|CHECK VIDEO ?? ht...|        Negative|Negative|\n",
      "|Breaking Story: O...|         Neutral| Neutral|\n",
      "+--------------------+----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPILTTING THE DATA INTO 3 SETS : \n",
    "# TRAINING SET\n",
    "# TEST SET\n",
    "# VALIDATION SET\n",
    "(train_set, val_set, test_set) = data.randomSplit([0.98, 0.01, 0.01], seed = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A PIPEPLINE OF 4 STAGES TO TRANFORM THE TWEET TEXT\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "# TOKENIZING THE TWEETS\n",
    "tokenizer = Tokenizer(inputCol=\"OriginalTweet\", outputCol=\"tokens\")\n",
    "\n",
    "# CALCULATING THE Term Frequency — Inverse Document Frequency FOR THE TWEET TEXT\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"tokens\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "# INDEXING THE TARGET SENTIMENT ACCORDING TO THE PROVIDED LABEL\n",
    "label_stringIdx = StringIndexer(inputCol = \"target_Sentiment\", outputCol = \"label\")\n",
    "\n",
    "# ASSEMBLING THE PIPELINE\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(train_set)\n",
    "train = pipelineFit.transform(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------------------+--------------------+--------------------+-----+\n",
      "|       OriginalTweet|target_Sentiment|              tokens|                  tf|            features|label|\n",
      "+--------------------+----------------+--------------------+--------------------+--------------------+-----+\n",
      "|    Police office...|        Positive|[, , , , police, ...|(65536,[1434,1511...|(65536,[1434,1511...|  0.0|\n",
      "|   I told them th...|        Negative|[, , , i, told, t...|(65536,[1198,5660...|(65536,[1198,5660...|  1.0|\n",
      "|  A revised rail ...|        Positive|[, , a, revised, ...|(65536,[463,1032,...|(65536,[463,1032,...|  0.0|\n",
      "|  Add your favori...|        Positive|[, , add, your, f...|(65536,[19208,203...|(65536,[19208,203...|  0.0|\n",
      "|  COVID 19 UPDATE...|        Positive|[, , covid, 19, u...|(65536,[3856,4629...|(65536,[3856,4629...|  0.0|\n",
      "+--------------------+----------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#transform model with validataion datasets\n",
    "val = pipelineFit.transform(val_set)\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLING LOGISTIC REGRESSION ON THE TRAINING SET\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "LR = LogisticRegression(maxIter=100)\n",
    "model = LR.fit(train)\n",
    "predictions = model.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.6807\n",
      "ROC-AUC: 0.7344\n"
     ]
    }
   ],
   "source": [
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7344114219114218"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
